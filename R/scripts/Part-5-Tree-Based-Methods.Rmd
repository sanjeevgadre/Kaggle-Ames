---
title: "Ames Housing Prices: Part V - Tree Based Methods"
author: "Sanjeev Gadre"
date: "June 28, 2019"
output: 
    md_document:
        toc: TRUE
        toc_depth: 3
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = FALSE)
```

Loading the required libraries.

```{r libraries, echo=TRUE, message=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(randomForest)
library(gbm)

```

### Getting the Data

1.  We start by getting the previously cleaned *train* subset.

```{r get-data}
train = readRDS("../RDA/train")

n = nrow(train)
p = ncol(train)-1
y = train$SalePrice %>% log()
```

### Introduction

1.  We will use the following 2 tree-based learning algorithms:
    a.  Random Forest Ensemble
    b.  Boosted Tree Ensemble
2.  We will use the *train-validate-test* strategy to fit an optimal model to the *train* data-set as well as estimate the likely test error.


### Random Forest Ensemble

1.  Since the *random forest* algorithm uses a *bootstrap* sampling strategy, the function `randomforest()` offers an easy built-in way to estimate test error without the need for an explicit *train-test* strategy.
2.  We iterate through different values of mumber of variables randomly sampled as candidates at each split allowing us to compare a random forest ensemble with a bagged tree ensemble in the same pass.
3.  For the two parameters to optimise we use the following sets
    a.  *number of trees grown*  - {100, 500, 2500}, and 
    b.  *number of features randomly chosen for a node split* - {p/9, p/3, p}, where is p is the number of features.
4.  (From the help file) For large data sets, especially those with large number of variables, calling randomForest via the formula interface is not advised: There may be too much overhead in handling the formula.

```{r randomforest}
x = model.matrix(SalePrice~., data = train)

forest.size = c(100, 500, 2500)
param.size = c(ncol(x)/9, ncol(x)/3, ncol(x)) %>% round(digits = 0)

est.test.err = (mean(y) - y)^2 %>% mean()
bestforestsize = 500; bestparamsize = ncol(x)/3
for (t in forest.size) {
    for (p in param.size) {
        set.seed(p)
        fit = randomForest(x, y, ntree = t, mtry = p, keep.forest = FALSE)
        test.err = fit$mse %>% min()
        if (test.err < est.test.err) {
            est.test.err = test.err
            bestforestsize = fit$mse %>% which.min()
            bestparamsize = p
        }
    }
}

est.test.err = round(est.test.err, digits = 4)

print(paste("The estimated mean squared test error for a random forest ensemble model =", est.test.err))
print(paste("For the least mean squared test errror, the optimal number of trees grown is", bestforestsize,
            "and the optimal number of parameters (for the model matrix) sampled at a node is", bestparamsize))

```

*Observations*

1.  In this particular instance, the random forest ensemble outperforms the bagged tree ensemble.

### Boosted Tree Ensemble

1.  The function `gbm()` offers an in-built *n*-fold cross validation functionality that allows for an easy way to estimate test error without the need for an explicit *train-test* strategy.
2.  We iterate through a series of values for the parameters `interaction depth` and `learning rate` to identify the boosted tree ensemble model with the least estimated mean squared test error.
3.  For the two parameters to optimise we use the following sets
    a.  *interaction depth*  - {1, 2, 3, 4}, and 
    b.  *learning rate* - {1, 0.3, 0.1, 0.03, 0.1}

```{r boosted-trees}
depth = 1:4
alpha = c(1, 0.3, 0.1, 0.03, 0.01)

est.test.err = (mean(y) - y)^2 %>% mean()
bestdepth = 1; bestalpha = 0.1
for (d in depth) {
    for (a in alpha) {
        set.seed(a)
        fit = gbm(log(SalePrice)~., data = train, distribution = "gaussian", n.trees = 2500, 
          interaction.depth = d, shrinkage = a, cv.folds = 10, keep.data = FALSE)
        
        test.err = fit$cv.error %>% min()
        if (test.err < est.test.err) {
            est.test.err = test.err
            bestdepth = d
            bestalpha = a
        }
    }
}

est.test.err = round(est.test.err, digits = 4)

print(paste("The estimated mean squared test error for a boosted forest ensemble model =", est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal interaction depth is", bestdepth,
            "and the optimal learning rate is", bestalpha))


```

*Observations*

1.  While the *Random Forest Ensemble* does not outperform the penalised regression models, the *Boosted Tree Ensemble* certainly does.