---
title: "Ames Housing Prices: Part IV - PC, PLS and Best Subset Regression "
author: "Sanjeev Gadre"
date: "June 28, 2019"
output: 
    html_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = FALSE)
```

Loading the required libraries.

```{r libraries, echo=TRUE, message=FALSE}
library(dplyr)
library(pls)
library(leaps)

```

### Getting the Data

1.  We start by getting the previously cleaned *train* subset.

```{r get-data, echo=TRUE}
train = readRDS("./RDA/train")

n = nrow(train)
p = ncol(train)-1
y = train$SalePrice %>% log()
```

### Introduction

1.  We will use the following 3 learning algorithms that help reduce the dimensionality of the train data:
    a.  Principal Component Regression
    b.  Partial Least Square Regression
    c.  Regresion using the best subset.
2.  We will use the *train-validate-test* strategy to fit an optimal model to the *train* data-set as well as estimate the likely test error.
3.  The *PCR* and *PLS* algorithms work best when data is scaled. However the `pcr` and `plsr` functions are cumbersome to use with unscaled data especially when the data has features of different types. So it is best that we scale the data before using the algorigthm.
4.  Only the `numeric` features can be scaled. Additionally, there are some features that while `numeric` in type have range of values that make them more like `fct` featuers and scaling needs to be avoided for these features.
5.  We first identify the `numeric` features. We then use the *range* of these `numeric` features to identify features that are better treated as `fct` and then scale only the remaining `numeric` features.
6.  Since we use `log(SalePrice)` as our dependent variable we must scale `log(SalePrice)` and not `SalePrice`.


```{r scaling}
train.scaled = train
train.scaled$SalePrice = log(train$SalePrice)

num.feat.indx = sapply(train, is.numeric)
num.feat.rnge = sapply(train[, num.feat.indx], range)

num.feat.like.fct = c("BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", 
                      "Fireplaces", "GarageCars")
num.feat.indx[num.feat.like.fct] = FALSE

train.scaled[, num.feat.indx] = apply(train.scaled[, num.feat.indx], 2, scale)

```

### Principal Component Regression

1.  We use the `pls` library's cross-validation function `crossval` for selection of optimal number of principal components and estimating the test error.

```{r pcr}
set.seed(1970)
fit = pcr(SalePrice~., data = train.scaled, ncomp = p, center = FALSE, validation = "none", model = FALSE)

set.seed(1970)
cv.out = crossval(fit, segments = 10, segment.type = "random")

opt.z = cv.out$validation$adj %>% which.min()

est.test.err = cv.out$validation$adj %>% min() %>% round(digits = 4)

print(paste("The estimated mean squared test error for a principal component regression model =", est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal number principal components = ",
            opt.z))

```


### Partial Least Square Regression

1.  We use the `pls` library's cross-validation function `crossval` for selection of optimal number of partial least square directions and estimating the test error.

```{r pls}
set.seed(1970)
fit = plsr(SalePrice~., data = train.scaled, ncomp = p, center = FALSE, validation = "none", model = FALSE)

set.seed(1970)
cv.out = crossval(fit, segments = 10, segment.type = "random")

opt.z = cv.out$validation$adj %>% which.min()

est.test.err = cv.out$validation$adj %>% min() %>% round(digits = 4)

print(paste("The estimated mean squared test error for a partial least square regression model =",
            est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal number partial least square directions = ", opt.z))

```

### Best Subset Selection - Forward Selection

1.  We perform a 10-fold cross validation to select the optimal size of the subset of features.
2.  The train data-set will be divided into 2 subsets - **train** and **test** - in the ratio 70:30. The *test subset* will be used to estimate the likely test error for a model buit using the optimal size of subset of features.

```{r regsubsets-forward}
no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)

est.val.err = rep(0, p)
for (i in 1:no.of.folds) {
    fit = regsubsets(log(SalePrice)~., data = train[indx != i, ], nvmax = p, method = "forward")
    
    test.mat = model.matrix(SalePrice~., data = train[indx == i,])
    
    for (j in 1:p) {
       coefj = coef(fit, id = j)
       y.val.hat = test.mat[, names(coefj)]%*%coefj
       val.err = (y.val.hat - y[indx == i])^2 %>% mean()
       est.val.err[j] = est.val.err[j] + val.err 
    }
    
}

est.val.err = est.val.err/no.of.folds

opt.z = which.min(est.val.err)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = regsubsets(log(SalePrice)~., data = train[indx, ], nvmax = opt.z, method = "forward")
    coef.fit = coef(fit, id = opt.z)
    test.mat = model.matrix(SalePrice~., data = train[-indx,])
    
    y.val.hat = test.mat[, names(coef.fit)]%*%coef.fit
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a best subset (forward) selection model =", est.test.err))

```

### Best Subset Selection - Backward Selection

1.  We use the same strategy as for the *Forward Selection* algorithm.

```{r regsubsets-backward}
no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)

est.val.err = rep(0, p)
for (i in 1:no.of.folds) {
    fit = regsubsets(log(SalePrice)~., data = train[indx != i, ], nvmax = p, method = "backward")
    
    test.mat = model.matrix(SalePrice~., data = train[indx == i,])
    
    for (j in 1:p) {
       coefj = coef(fit, id = j)
       y.val.hat = test.mat[, names(coefj)]%*%coefj
       val.err = (y.val.hat - y[indx == i])^2 %>% mean()
       est.val.err[j] = est.val.err[j] + val.err 
    }
    
}

est.val.err = est.val.err/no.of.folds

opt.z = which.min(est.val.err)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = regsubsets(log(SalePrice)~., data = train[indx, ], nvmax = opt.z, method = "backward")
    coef.fit = coef(fit, id = opt.z)
    test.mat = model.matrix(SalePrice~., data = train[-indx,])
    
    y.val.hat = test.mat[, names(coef.fit)]%*%coef.fit
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a best subset (backward) selection model =", est.test.err))

```

*Observations*
1.  None of the dimensionality reducing regression models outperform the penalised regression models.



