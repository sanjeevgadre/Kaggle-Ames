---
title: "Ames Housing Prices: Part III - Penalised Regression"
author: "Sanjeev Gadre"
date: "June 28, 2019"
output: 
    md_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE)
```

Loading the required libraries.

```{r libraries, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(glmnet)
library(gridExtra)

```

### Getting the Data

1.  We start by getting the previously cleaned *train* subset.

```{r get-data}
train = readRDS("./RDA/train")

```

### Introduction

1.  We will build both the ridge penalised and lasso penalised regression models.
2.  The `glmnet` package offers the `cv.glmnet()` function which allows a simple method to perform a n-fold cross validation to estimate the mean squared test error.
3.  We first establish a baseline by estimating the likely test error for an un-penalised linear regresssion model.

```{r common}
n = nrow(train)
p = ncol(train) - 1
y = train$SalePrice %>% log()
x = model.matrix(SalePrice~., data = train)[,-1]

```

### Unpenalised Linear Regression

```{r unpenalised}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 0, lambda = c(10^-10, 0))
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a un penalised linear regression model =", est.test.err))

```

### Ridge Penalised Linear Regression

```{r ridge}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a ridge penalised linear regression model =", est.test.err))

```

### Lasso Penalised Linear Regression

```{r lasso}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a lasso penalised linear regression model =", est.test.err))

```

*Observations*

1.  The two penalised regression models perform about the same when using estimated mean squared test error as the metric of measurement.
