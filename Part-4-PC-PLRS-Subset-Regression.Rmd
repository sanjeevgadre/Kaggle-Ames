---
title: "Ames Housing Prices: Part IV - PC, PLS and Subset Regression "
author: "Sanjeev Gadre"
date: "June 28, 2019"
output: 
    md_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE)
```

Loading the required libraries.

```{r libraries, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(glmnet)
library(gridExtra)
library(pls)
library(leaps)

```

Loading required functions.

```{r functions}
na.count = function (dat){
  dat %>% apply(., 2, is.na) %>% apply(.,2,sum) %>% .[.!=0]
}

```

### Getting the Data

1.  We start by getting the previously cleaned *train* subset.

```{r get-data}
train = readRDS("./RDA/train")

```

### Introduction

1.  We will use the following 3 learning algorithms that help reduce the dimensionality of the train data:
    a.  Principal Component Regression
    b.  Partial Least Square Regression
    c.  Regresion using the best subset.
2.  We will use the *train-validate-test* strategy to fit an optimal model to the *train* data-set as well as estimate the likely test error.
3.  The train data-set will be divided into 2 subsets - **train** and **test** - in the ratio 70:30.
4.  The *train* subset will be used to build the model. The `pls` library has an inbuilt cross-validation feature that allows for selection of optimal number of principal components and partial least square directions.
5.  The *test subset* will be used to estimate the likely test error.

### Principal Component Regression

1.  We do a 5-fold cross validation using the *train* subset to determine the optimal number of principal components to use when fitting a principal component regression model.
2.  We then use the 

```{r PCR}
n = nrow(train)
x = model.matrix(SalePrice~., data = train)
y = train$SalePrice %>% log()

no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)
est.val.err = rep(0, ncol(x))
for (i in 1:no.of.folds) {
    fit = svdpc.fit(x[indx != i,], y[indx != i], ncomp = ncol(x), center = TRUE, scale = TRUE, stripped = TRUE)
    
    fit.coeff = array(fit$coefficients[,1,], dim = c(ncol(x), ncol(x)))
    
    y.val.hat = x[indx == i,]%*%fit.coeff
    val.err = (y.val.hat - y[indx == i])^2 %>% apply(., 2, mean)
    est.val.err = est.val.err + val.err
}

est.val.err = est.val.err/no.of.folds

z = est.val.err %>% which.min()

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = svdpc.fit(x[indx,], y[indx], ncomp = z, center = TRUE, scale = TRUE, stripped = TRUE)
    
    fit.coeff = fit$coefficients[, 1, z]
    
    y.val.hat = x[-indx,]%*%fit.coeff %>% as.numeric()
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a principal component regression model =", est.test.err))

```

### Partial Least Square Regression

```{r pls}
n = nrow(train)
x = model.matrix(SalePrice~., data = train)
y = train$SalePrice %>% log()

no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)
est.val.err = rep(0, ncol(x))
for (i in 1:no.of.folds) {
    fit = oscorespls.fit(x[indx != i,], y[indx != i], ncomp = ncol(x), center = TRUE, stripped = TRUE)

    fit.coeff = array(fit$coefficients[,1,], dim = c(ncol(x), ncol(x)))

    y.val.hat = x[indx == i,]%*%fit.coeff
    val.err = (y.val.hat - y[indx == i])^2 %>% apply(., 2, mean)
    est.val.err = est.val.err + val.err
}

est.val.err = est.val.err/no.of.folds

z = est.val.err %>% which.min()

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = oscorespls.fit(x[indx,], y[indx], ncomp = z, center = TRUE, stripped = TRUE)
    
    fit.coeff = fit$coefficients[, 1, z]
    
    y.val.hat = x[-indx,]%*%fit.coeff %>% as.numeric()
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a partial least square regression model =", est.test.err))
```

### Best Subset Selection

```{r regsubsets}
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)

for (i in 1:no.of.folds) {
    fit = regsubsets(log(SalePrice)~., data = train[indx != i, ], nvmax = p, really.big = TRUE)
    
    
    
    
    
    fit = regs(x[indx != i,], y[indx != i], ncomp = ncol(x), center = TRUE, stripped = TRUE)

    fit.coeff = array(fit$coefficients[,1,], dim = c(ncol(x), ncol(x)))

    y.val.hat = x[indx == i,]%*%fit.coeff
    val.err = (y.val.hat - y[indx == i])^2 %>% apply(., 2, mean)
    est.val.err = est.val.err + val.err
}

```

