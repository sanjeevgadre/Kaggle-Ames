---
title: "Ames-Housing-Data: Experiments with ML Algorigthms"
author: "Sanjeev Gadre"
date: "Spetember 13, 2019"
output: 
    html_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

Loading the required libraries.

```{r libraries, echo=TRUE, message=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(glmnet)
library(pls)
library(leaps)
library(randomForest)
library(gbm)

```

Loading required functions.

```{r functions, echo=TRUE}
na.count = function (dat){
  dat %>% apply(., 2, is.na) %>% apply(.,2,sum) %>% .[.!=0]
}

```

### Pre-processing Data, Exploratory Data Analysis & Imputing Missing Data

####    Getting the Data

1.  We add a column `SalePrice` to the *test* dataset and assign a default value of "1000".
2.  We combide *train* and *test* datasets, for easier changes to the dataframe structure, imputing missing values and analysis.
3.  We start by getting a sense of the *train* dataset and its structure.

```{r get-data-1}
train = read.csv("./data/train.csv")
test = read.csv("./data/test.csv")
test$SalePrice = 1000

dat = rbind(train, test)

indx = nrow(train)

print("Sample examples from the train dataset")
head(train)
print("Structure of the train dataset")
str(train)

```

####    Pre-Process - I

1.  We change the data type for a few features to better align it with the type of feature data. Specifically, we convert  `MSSubClass`, `OverallQual`, `OverallCond` from `<int>` to `<fctr>`.
2.  We identify the features in the *train* dataset that report `NA` values

```{r pre-proc-1}
dat$MSSubClass = factor(dat$MSSubClass)
dat$OverallCond = factor(dat$OverallCond)
dat$OverallQual = factor(dat$OverallQual)

print("Features in train dataset reporting NA values")
na.count(dat[1:indx,])

```

**Observations**

Some features are *incorrectly* coded in that `NA` doesn't mean missing data but has a an alternate meaning. We
convert `NA` to a more appropriate values for the following features:

1.  When a house has *No Alley Access*, it is recorded under the column `Alley` as `NA` which is corrected to   `NoAccess`.
2.  When a house has *No Basement*, it is recorded under the columns `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2` as `NA` which is corrected to `NoBsmt`.
3.  When a house has *No Fireplace*, it is recorded under the column `FireplaceQu` as `NA` which is corrected to `NoFpl`.
4.  When a house has *No Garage*, it is recorded under the columns `GarageType`, `GarageFinish`, `GarageQual`, `GarageCond` as `NA` which is corrected to `NoGrg`.
5.  When a house has *No Pool*, it is recorded under the column `PoolQc` as `NA` which is corrected to        `NoPool`.
6.  When a house has *No Fence*, it is recorded under the column `Fence` as `NA` which is corrected to            `NoFence`.
7.  When a house has *No Miscellaneous Features*, it is recorded under the column `MiscFeature` as `NA` which is corrected to `NoMisc`.

We apply these changes to both *train* and *test* datasets.

8.  We identify the features in the *train* dataset that are reporting `NA`.

####    Pre-Process - II

```{r pre-proc-2}
levels(dat$Alley) = c(levels(dat$Alley), "NoAccess")
dat[is.na(dat$Alley), "Alley"] = "NoAccess"
    
levels(dat$BsmtQual) = c(levels(dat$BsmtQual), "NoBsmt")
dat[is.na(dat$BsmtQual), "BsmtQual"] = "NoBsmt"  
    
levels(dat$BsmtCond) = c(levels(dat$BsmtCond), "NoBsmt")
dat[is.na(dat$BsmtCond), "BsmtCond"] = "NoBsmt"
    
levels(dat$BsmtExposure) = c(levels(dat$BsmtExposure), "NoBsmt")
dat[is.na(dat$BsmtExposure), "BsmtExposure"] = "NoBsmt"
    
levels(dat$BsmtFinType1) = c(levels(dat$BsmtFinType1), "NoBsmt")
dat[is.na(dat$BsmtFinType1), "BsmtFinType1"] = "NoBsmt"
    
levels(dat$BsmtFinType2) = c(levels(dat$BsmtFinType2), "NoBsmt")
dat[is.na(dat$BsmtFinType2), "BsmtFinType2"] = "NoBsmt"
    
levels(dat$FireplaceQu) = c(levels(dat$FireplaceQu), "NoFpl")
dat[is.na(dat$FireplaceQu), "FireplaceQu"] = "NoFpl"
    
levels(dat$GarageType) = c(levels(dat$GarageType), "NoGrg")
dat[is.na(dat$GarageType), "GarageType"] = "NoGrg"
    
levels(dat$GarageFinish) = c(levels(dat$GarageFinish), "NoGrg")
dat[is.na(dat$GarageFinish), "GarageFinish"] = "NoGrg"
    
levels(dat$GarageQual) = c(levels(dat$GarageQual), "NoGrg")
dat[is.na(dat$GarageQual), "GarageQual"] = "NoGrg"
    
levels(dat$GarageCond) = c(levels(dat$GarageCond), "NoGrg")
dat[is.na(dat$GarageCond), "GarageCond"] = "NoGrg"
    
levels(dat$PoolQC) = c(levels(dat$PoolQC), "NoPool")
dat[is.na(dat$PoolQC), "PoolQC"] = "NoPool"
    
levels(dat$Fence) = c(levels(dat$Fence), "NoFence")
dat[is.na(dat$Fence), "Fence"] = "NoFence"
    
levels(dat$MiscFeature) = c(levels(dat$MiscFeature), "NoMisc")
dat[is.na(dat$MiscFeature), "MiscFeature"] = "NoMisc"

print("Features in train dataset reporting NA values")
na.count(dat[1:indx,])

```

####    Exploratory Data Analysis - I

We get additional visibility to the features in the *train* dataset reporting `NA` values

```{r eda-1}
writeLines("Property Ids reporting NA for GarageYrBlt")
dat[1:indx,] %>% .[is.na(.$GarageYrBlt),c("Id", "GarageType", "GarageYrBlt")]

writeLines("Distribution of types of 'Elec. System' used")
table(dat$Electrical[1:indx], dnn = "Elec. System Used") %>% prop.table() %>% round(digits = 4)

writeLines("Distribution of types of 'Masonry Veneer' used")
table(dat$MasVnrType[1:indx], dnn = "Masonry Veneer Type") %>% prop.table() %>% round(digits = 4)

writeLines("Distribution of 'Masonry Veneer Area'")
hist(dat$MasVnrArea[1:indx], xlab = "Area", main = "Histogram of MasVnrArea")

writeLines("Distribution of 'Lot Area' used\n")
hist(dat$LotFrontage[1:indx], xlab = "Area", main = "Histogram of LotArea")

```

**Observations:**

1.  All 81 properties reporting `NA` for `GarageYrBlt` also report that they have no garage. 
2.  The most common value of `Electrical` is `SBrkr` and for `MasVnrType` is `None`
3.  The distribution for `LotFrontage` and `MasVnrArea` is left skewed.

####    Impute Missing Data - I

We use the insights above to impute missing values to both the *train* and *test* datasets for the 5 features analysed above

1.  We make an experience-based assumption that a property's value is dependent on the presence or absence of a garage rather that the age of the garage. As such, we *drop* the feature `GarageYrBlt` as the information of the presence or absence of a garage is already captured in `GarageType`.
2.  To the `NA` in `Electrical` and `MasVnrType` we impute the mode value of the respective feature based on the remaining train examples.
3.  To the `NA` in `MasVnrArea` and `LotFrontage` we impute the median value of the respective feature based on the remaining train examples.
4.  We verify that there indeed are no more missing values in the *train* dataset.
5.  Finally we identify the missing values in the *test* dataset.

It is important to note that we **only** use the *train* examples to decide on the median and mode values to impute.

```{r impute-data-1}
dat[is.na(dat$Electrical), "Electrical"] = dat$Electrical[1:indx] %>% table() %>% which.max() %>% names()
dat[is.na(dat$MasVnrType), "MasVnrType"] = dat$MasVnrType[1:indx] %>% table() %>% which.max() %>% names()
dat[is.na(dat$MasVnrArea), "MasVnrArea"] = dat$MasVnrArea[1:indx] %>% median(., na.rm = TRUE)
dat[is.na(dat$LotFrontage), "LotFrontage"] = dat$LotFrontage[1:indx] %>% median(., na.rm = TRUE)

dat = subset(dat, select = -GarageYrBlt)

print("Features in train dataset reporting NA values")
na.count(dat[1:indx,])

print("Features in test dataset reporting NA values")
na.count(dat[(indx+1):nrow(dat),])

```

####    Exploratory Data Analysis - II

1.  We get some additional visibility to the examples in the *test* dataset reporting `NA`; specifically we ascertain if there are examples reporting `NA` under multiple features.

```{r eda-2}
dat[is.na(dat$MSZoning), c("Id", "Neighborhood", "MSZoning")]
dat[is.na(dat$Utilities), c("Id", "Neighborhood", "Utilities")]
dat[is.na(dat$Exterior1st), c("Id", "MSSubClass", "Exterior1st", "Exterior2nd")]
dat[is.na(dat$BsmtFinSF1), c("Id", "MSSubClass", "BsmtQual", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", 
                             "TotalBsmtSF")]
dat[is.na(dat$BsmtFullBath), c("Id", "MSSubClass", "BsmtQual", "BsmtFullBath", "BsmtHalfBath")]
dat[is.na(dat$KitchenQual), c("Id", "MSSubClass", "KitchenQual")]
dat[is.na(dat$Functional), c("Id", "MSSubClass", "Functional")]
dat[is.na(dat$GarageCars), c("Id", "GarageType", "GarageCars", "GarageArea")]
dat[is.na(dat$SaleType), c("Id", "MSSubClass", "SaleType")]

```

**Observations**

1.  A single exampe 2152 reports `NA` for both `Exterior1st` and `Exterior2nd`. 
2.  Example ids. 2121 and 2189 both report absence of a basement and hence we can impute `0` to all related features like `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath` and `BsmtHalfBath`.
3.  A single example 2577 reports `NA` for both `GarageCars` and `GarageArea`.

**Insights**

1.  We make experience based hypotheses that features:
    a.  `MSZoning` and `Utilities` depend on the `Neighborhood`,
    b.  `Exterior1st`, `Exterior2nd`, `KitchenQual`, `Functional` and `SaleType` depend on the type of house (`MSSubClass`), and 
    c.  `GarageCars` and `GarageArea` depend on typle of Garage (`GarageType`).
2.  To validate this assumption, we look at the distribution of:
    a.  `MSZoning` and `Utilities` for different `Neighborhood`
    b.  `Exterior1st`, `Exterior2nd`, `KitchenQual`, `Functional` and `SaleType` for different `MSSubclass`, and
    c.  `GarageCars` and `GarageArea` for different `GarageType`. 

We look at these distributions only for the *train* examples.

####    Exploratory Data Analysis - III

```{r eda-3}
writeLines("The first table of each pair of tables below show the overall distribution of a feature across all examples and the second one the distribution of the feature stratified by 'MSSubClass'\n")

table(MSZoning = dat$MSZoning[1:indx], dnn = "MSZoning") %>% prop.table() %>% round(digits = 4)
table(MSZoning = dat$MSZoning[1:indx], Neighborhood = dat$Neighborhood[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(Utilities = dat$Utilities[1:indx], dnn = "Utilities") %>% prop.table() %>% round(digits = 4)
table(Utilities = dat$Utilities[1:indx], Neighborhood = dat$Neighborhood[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(Xterior1 = dat$Exterior1st[1:indx], dnn = "Exterior1st") %>% prop.table() %>% round(digits = 4)
table(Xterior1 = dat$Exterior1st[1:indx], MSSubclass = dat$MSSubClass[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(Xterior2 = dat$Exterior2nd[1:indx], dnn = "Exterior2nd") %>% prop.table() %>% round(digits = 4)
table(Xterior2 = dat$Exterior2nd[1:indx], MSSubclass = dat$MSSubClass[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(KitchenQual = dat$KitchenQual[1:indx], dnn = "KitchenQual") %>% prop.table() %>% round(digits = 4)
table(KitchenQual = dat$KitchenQual[1:indx], MSSubclass = dat$MSSubClass[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(Functional = dat$Functional[1:indx], dnn = "Functional") %>% prop.table() %>% round(digits = 4)
table(Functional = dat$Functional[1:indx], MSSubclass = dat$MSSubClass[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(SaleType = dat$SaleType[1:indx], dnn = "SaleType") %>% prop.table() %>% round(digits = 4)
table(SaleType = dat$SaleType[1:indx], MSSubclass = dat$MSSubClass[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)
writeLines("\n")
table(GarageCars = dat$GarageCars[1:indx], dnn = "GarageCars") %>% prop.table() %>% round(digits = 4)
table(GarageCars = dat$GarageCars[1:indx], GarageType = dat$GarageType[1:indx]) %>% 
    prop.table(margin = 2) %>% round(digits = 4)

garage.plot.1 = dat[1:indx,] %>% filter(.$GarageType != "NoGrg") %>% 
    ggplot(aes(x = GarageArea))+ geom_density()+ 
    labs(title = "Overall Garage Area Distribution", x = "Area in sq.ft", y = "")
garage.plot.2 = dat[1:indx,] %>% filter(.$GarageType != "NoGrg") %>% ggplot(aes(x = GarageArea))+ 
    geom_density()+ facet_grid(GarageType~.)+ 
    labs(title = "Area Distribution by Garage Type", x = "Area in sq.ft", y = "")
grid.arrange(garage.plot.1, garage.plot.2, ncol = 2)

```

**Observations**

1.  The tables and the graphs above validate our hypotheses that, features - 
    a.  `MSZoning` and `Utilities` depend on `Neighborhood`
    b.  `Exterior1st`, `Exterior2nd`, `KitchenQual`, `Functional` and `SaleType` depend on the type of house (`MSSubClass`), and 
    c.  `GarageCars` and `GarageArea` depend on typle of Garage (`GarageType`).

####    Impute Missing Data - II

1.  We impute 
    a.  the mode values for missing `MSZoning` and `Utilities` of appropriate `Neighborhood`
    b.  the mode values for missing `Exterior1st`, `Exterior2nd`, `KitchenQual`, `Functional` and `SaleType` of appropriate `MSSubClass`, and 
    c.  the mode and median values respectively for missing `GarageCars` and `GarageArea` of appropriate `GarageType`
2.  Example ids. 2121 and 2189 both report absence of a basement and hence we can impute `0` to all related features like `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath` and `BsmtHalfBath`.
3.  Finally, we reconfirm that there are no more missing values.

```{r impute-data-2}
#   MSZoning
x = dat$MSZoning[dat$Neighborhood == "IDOTRR"] %>% table() %>% which.max() %>% names()
dat$MSZoning[c(1916, 2217, 2251)] = x
x = dat$MSZoning[dat$Neighborhood == "Mitchel"] %>% table() %>% which.max() %>% names()
dat$MSZoning[2905] = x
#   Utilities
x = dat$Utilities[dat$Neighborhood == "IDOTRR"] %>% table() %>% which.max() %>% names()
dat$Utilities[1916] = x
x = dat$Utilities[dat$Neighborhood == "Gilbert"] %>% table() %>% which.max() %>% names()
dat$Utilities[1946] = x

#   Exterior1st
x = dat$Exterior1st[dat$MSSubClass == 30] %>% table() %>% which.max() %>% names()
dat$Exterior1st[2152] = x

#   Exterior2nd
x = dat$Exterior2nd[dat$MSSubClass == 30] %>% table() %>% which.max() %>% names()
dat$Exterior2nd[2152] = x

#   KitchenQual
x = dat$KitchenQual[dat$MSSubClass == 50] %>% table() %>% which.max() %>% names()
dat$KitchenQual[1556] = x

#   Functional
x = dat$Functional[dat$MSSubClass == 20] %>% table() %>% which.max() %>% names()
dat$Functional[2217] = x
x = dat$Functional[dat$MSSubClass == 50] %>% table() %>% which.max() %>% names()
dat$Functional[2474] = x

#   SaleType
x = dat$SaleType[dat$MSSubClass == 20] %>% table() %>% which.max() %>% names()
dat$SaleType[2490] = x

#   GarageCars
x = dat$GarageCars[dat$GarageType == "Detchd"] %>% median(na.rm = TRUE)
dat$GarageCars[2577] = x

#   GarageArea
x = dat$GarageArea[dat$GarageType == "Detchd"] %>% median(na.rm = TRUE)
dat$GarageArea[2577] = x

#   BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath.
dat[c(2121, 2189), c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath")] =
    0

print("Features in test dataset reporting NA values")
na.count(dat[(indx+1):nrow(dat),])

```

####    Saving a Baseline

1.  We have "cleaned" both the *train* and *test* datasets.
2.  We assign the `Id` as rownames for the examples and then drop the `Id` feature.
3.  Finally, we separate the *train* and the *test* datasets and save them as *.Rda* files as baseline versions

```{r baseline-1, echo=TRUE}
rownames(dat) = dat$Id
dat = subset(dat, select = -Id)
train = dat[1:indx,]
test = dat[(indx+1):nrow(dat),]

saveRDS(train, "./RDA/train")
saveRDS(test, "./RDA/test")

#   Cleanup
rm(list = ls())

```

### Data Transformation and Learning Curves

####    Getting the Data

1.  We start by getting the previously cleaned *train* dataset.

```{r get-data-2, echo=TRUE}
train = readRDS("./RDA/train")

```

####    Investigating Likely Heteroscedacity

1.  To investigate heterscedacity we plot the **Studentized Residuals** against the dependent variable, `SalePrice`, for all the examples.

```{r homoscedacity-1}
fit = lm(SalePrice~., data = train)

res.df = data.frame(DepVar = train$SalePrice, StndRes = fit$residuals/sd(fit$residuals))

res.df %>% ggplot(aes(DepVar, StndRes))+ geom_point()+ 
    labs(title = " ", x = "Sale Price", y = "Studentized Residuals")

```

**Observations**

1.  As `SalePrice` gets larger, the studentized residuals of the linear model also tend to get larger and this trend indicates potential heteroscedacity in the response variable. 
2.  We attempt to eliminate this heteroscedacity by using the log values of the dependent variable in building the linear fit model.
3.  To ascertain if we have eliminated heteroscedacity we once again plot the studentized residuals against the *new* dependent variable.

```{r homoscedacity-2}
fit = lm(log(SalePrice)~., data = train)
res.df$DepVar = log(train$SalePrice)
res.df$StndRes = fit$residuals
res.df %>% ggplot(aes(DepVar, StndRes))+ geom_point()+ 
    labs(title = " ", x = "Log of Sale Price", y = "Studentized Residuals")

```

**Observations**

1.  We see that the heteroscedacity in the response variable is now eliminated and therefore conclude that we should **use the log value of the dependent variable** (`Sale Price`) when building a model.

####    Identifying *Extreme* Examples

1.  We identify *high-levarage* and *outlier* examples. To that end we use the following hurdles:
    a.  An example is a high-leverage point if its *leverage statistic is 10 times of more than the average leverage* for all observations.
    b.  An example is an outlier if its *studentized residual is greater than 3*.
2.  We fit a linear regression model to the data and use the model statistics to identify the extreme examples.

```{r xtrm-ex-1}
res.df$LvrgStat = hatvalues(fit)
rownames(res.df) = rownames(train)

#   Hurdle for a high leverage datapoint 
#   Avergae high-leverage for the data-set = ((number of features)+1)/(number of examples)
h.stat.hrdl = 10*(ncol(train)/nrow(train))

#   Hurdle for outlier point
outlier.hrdl = 3

res.df = res.df %>% 
    mutate(., cndt = ifelse(res.df$StndRes > outlier.hrdl & res.df$LvrgStat >= h.stat.hrdl, "Both", 
                            ifelse(res.df$StndRes > outlier.hrdl, "Outlier", 
                                   ifelse(res.df$LvrgStat >= h.stat.hrdl, "HiLeverage", "None"))))

print("The number of type of extreme examples identified")
table(res.df$cndt)

```

**Observations**

1.  There are 75 data point that are likely hi-leverage. 
    a.  We consider the impact on the quality of fit after excluding each one of them individually from the dataset to which the linear model is fitted.
2.  We first establish a baseline for cross-validated estimated test error for the linear model using all data points. We then compare the improvement in this measure when each of these 75 data points are excluded individually and the penalised linear model is refitted.
3.  We use the `glmnet` package as there is a ready function to calculate cross-validated estimated test error.To *mimic* a non-penalised linear fit, we use extremely small values for the penalty factor lambda.
4.  We identify those candidate data points that reduce the MSE by 20% i.e. mean error ~10%.

```{r xtrm-ex-2}
x = model.matrix(data = train, SalePrice~.)[,-1]
y = train$SalePrice %>% log()
cndt = rownames(res.df[(res.df$cndt != "None"),]) %>% as.integer()

set.seed(1970)
cv.mse.base = cv.glmnet(x, y, lambda = c(10^-10,0))$cvm[1]

cv.mse.cndt = data.frame(Id = cndt, MSE = c(rep(0, length(cndt))))
i=1
#   This loop will take some time to complete
for (c in cndt) { 
    set.seed(1970) 
    cv.mse.cndt[i,2] = cv.glmnet(x[-c,], y[-c], lambda = c(10^-10,0))$cvm[1] 
    i = i+1
}

cv.mse.cndt = cv.mse.cndt %>% mutate(ratio = cv.mse.cndt$MSE/cv.mse.base)
cv.mse.cndt = cv.mse.cndt %>% mutate(cndt = ifelse(ratio < 0.8, TRUE, FALSE))

cv.mse.cndt[cv.mse.cndt$cndt == TRUE, c("Id", "ratio")]

cndt = cv.mse.cndt[cv.mse.cndt$cndt == TRUE, "Id"]

print("Candidate data points that reduce the MSE by 20% i.e. mean error ~10%")
res.df[cndt,]

```

**Observations**

1.  We find that 3 data points, Id nos. 272, 1276 and 1299, when excluded individually from the data used to model a linear fit, reduce the baseline model's mean squared error by over 20% (i.e. reduce the error by over 10.6%). These three points should therefore be excluded from building any model using the train subset.
2.  Further we see that these points are likely hi-leverage points which further strengthens the decision to exclude them when building the model.

####    Learning Curves

1.  Before proceeding further with feature engineering, it would be worthwhile to draw some *Learning Curves* to determine if a basic logistical regression model suffers from high bias or high variance. This will inform our feature engineering better.
2.  We divide the *train* dataset into a *train* subset and a *cval* subset in the ratio of 70:30. We shuffle the train dataset 5 times to improve our estimates for the learning curve.
3.   In forming the *Learning Curves*,we use small sized subsets of the train data and it is likely that for certain factor variables all levels may not be represented in the *train* subset or the *cval* subset. This causes error in the `predict.lm` function and we therefore need to use the lower-level `lm.fit` function when fitting a linear model. This requires that the independent variables be provided in a model matrix format.

```{r learning-curves}
train = train[-cndt,]

lc.steps = 25
err.train = rep(0, lc.steps); err.val = err.train

for (k in 1:5) {
    set.seed(k)
    indx = sample(nrow(train), 0.7*nrow(train), replace = FALSE)
    lc.step.size = length(indx)/lc.steps
    lc.set.size = seq(lc.step.size, length(indx), length.out = lc.steps) %>% round(digits = 0)
    
   
    x = model.matrix(SalePrice~., data = train)
    y = train$SalePrice %>% log()
    
    for (i in 1:lc.steps) {
        x.subset = x[indx[1:lc.set.size[i]],]; y.subset = y[indx[1:lc.set.size[i]]]
        fit = lm.fit(x.subset, y.subset)
        err.train[i] = err.train[i] + sqrt(mean(fit$residuals^2))
        
        fit.coeff = fit$coefficients
        fit.coeff = ifelse(is.na(fit.coeff), 0, fit.coeff)
        
        y.val = y[-indx]
        y.val.hat = x[-indx,]%*%fit.coeff %>% as.numeric()
        err.val[i] = err.val[i] + sqrt(mean((y.val.hat - y.val)^2))
    }
}

err.train = err.train/5; err.val = err.val/5
err.df = cbind(Size = lc.set.size, Train = err.train, Val = err.val, err.ratio = err.train/err.val) %>%
    as.data.frame()

lc.plot.1 = err.df %>% ggplot(aes(x = Size))+ 
    geom_line(aes(y = Train, color = "Train.Subset.Error"), na.rm = TRUE)+ 
    geom_line(aes(y = Val, color = "CV.Subset.Error"), na.rm = TRUE)+ 
    labs(title = "Learning Curves", x = "Train Subset Sample Size", y = "RMSE")+
    scale_colour_manual(name="Key", values=c(Train.Subset.Error ="red", CV.Subset.Error ="blue"))+ 
    theme(legend.position = "bottom")+ ylim(0, 0.5)

lc.plot.2 = err.df %>% ggplot(aes(x = Size, y = err.ratio))+ geom_line(na.rm = TRUE)+
    labs(title = "Learning Curves", x = "Train Subset Sample Size", y = "Ratio of Train Error to Cval Error")+
    ylim(0,1)

grid.arrange(lc.plot.1, lc.plot.2, ncol = 2)

```

**Observation**

1.  The *train* subset error and *cval* subset error curves are trending closer to each other but the gap is continually narrowing as the size of the *train* subest increases. This is even more apparent in the graph on the right. This indicates that a linear regression fit is likely to suffer from high variance.
2.  We have two options - either increase the number of train examples or reduce the dimensions of the *train* data-set. Clearly the former is not an option and so we propose to use learning algorithms that reduce the dimensionality of the *train* dataset.
3.  We propose to use the following learning algorithms:
    a.  Ridge-penalised linear regression
    b.  Lasso-penalised linear regression
    c.  Linear regression using prinicipal components
    d.  Linear regression using partial least square
4.  Additionally, we propose to use 2 non-parametric, tree based learning algorithms:
    a.  Random Forest ensemble
    b.  Boosted Tree ensemble

####    Saving a Baseline

1.  We save a baseline of the updated *train* dataset.

```{r baseline-2, echo=TRUE}
saveRDS(train, "./RDA/train")

#   Cleanup
rm(list = ls())

```

### Penalised Regression Models

####    Introduction

1.  We will build both the ridge penalised and lasso penalised regression models.
2.  The `glmnet` package offers the `cv.glmnet()` function which allows a simple method to perform a n-fold cross validation to estimate the mean squared test error.
3.  We first establish a baseline by estimating the likely test error for an un-penalised linear regresssion model.

####    Getting the Data

1.  Setting up the required data.

```{r get-data-3, echo=TRUE}
train = readRDS("./RDA/train")
n = nrow(train)
p = ncol(train) - 1
y = train$SalePrice %>% log()
x = model.matrix(SalePrice~., data = train)[,-1]

```

####    Unpenalised Linear Regression

```{r unpenalised}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 0, lambda = c(10^-10, 0))
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a un penalised linear regression model =", est.test.err))

```

####    Ridge Penalised Linear Regression

```{r ridge}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a ridge penalised linear regression model =", est.test.err))

```

####    Lasso Penalised Linear Regression

```{r lasso}
est.test.err = 0; bestlambda = rep(0, 5)
for (i in 1:5) {
    set.seed(i)
    cv.out = cv.glmnet(x, y, type.measure = "mse", alpha = 1)
    test.err = cv.out$cvm %>% min()
    est.test.err = est.test.err + test.err
} 

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a lasso penalised linear regression model =", est.test.err))

```

*Observations*

1.  The two penalised regression models perform about the same when using estimated mean squared test error as the metric of measurement.

```{r baseline-3, echo=TRUE}
#   Cleanup
rm(list = ls())

```

### Other Dimensionality Reducing Regression Models

####    Introduction

1.  We will use the following 3 learning algorithms that help reduce the dimensionality of the train data:
    a.  Principal Component Regression
    b.  Partial Least Square Regression
    c.  Regresion using the best subset.
2.  We will use the *train-validate-test* strategy to fit an optimal model to the *train* data-set as well as estimate the likely test error.
3.  The *PCR* and *PLS* algorithms work best when data is scaled. However the `pcr` and `plsr` functions are cumbersome to use with unscaled data especially when the data has features of different types. So it is best that we scale the data before using the algorigthm.
4.  Only the `numeric` features can be scaled. Additionally, there are some features that while `numeric` in type have range of values that make them more like `fct` featuers and scaling needs to be avoided for these features.
5.  We first identify the `numeric` features. We then use the *range* of these `numeric` features to identify features that are better treated as `fct` and then scale only the remaining `numeric` features.
6.  Since we use `log(SalePrice)` as our dependent variable we must scale `log(SalePrice)` and not `SalePrice`.

####    Getting the Data

1.  Setting up the required data.

```{r get-data-4, echo=TRUE}
train = readRDS("./RDA/train")

n = nrow(train)
p = ncol(train)-1
y = train$SalePrice %>% log()

train.scaled = train
train.scaled$SalePrice = log(train$SalePrice)

num.feat.indx = sapply(train, is.numeric)
num.feat.rnge = sapply(train[, num.feat.indx], range)

num.feat.like.fct = c("BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", 
                      "Fireplaces", "GarageCars")
num.feat.indx[num.feat.like.fct] = FALSE

train.scaled[, num.feat.indx] = apply(train.scaled[, num.feat.indx], 2, scale)

```

#####   Principal Component Regression

1.  We use the `pls` library's cross-validation function `crossval` for selection of optimal number of principal components and estimating the test error.

```{r pcr}
set.seed(1970)
fit = pcr(SalePrice~., data = train.scaled, ncomp = p, center = FALSE, validation = "none", model = FALSE)

set.seed(1970)
cv.out = crossval(fit, segments = 10, segment.type = "random")

opt.z = cv.out$validation$adj %>% which.min()

est.test.err = cv.out$validation$adj %>% min() %>% round(digits = 4)

print(paste("The estimated mean squared test error for a principal component regression model =", est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal number principal components = ",
            opt.z))

```

####    Partial Least Square Regression

1.  We use the `pls` library's cross-validation function `crossval` for selection of optimal number of partial least square directions and estimating the test error.

```{r pls}
set.seed(1970)
fit = plsr(SalePrice~., data = train.scaled, ncomp = p, center = FALSE, validation = "none", model = FALSE)

set.seed(1970)
cv.out = crossval(fit, segments = 10, segment.type = "random")

opt.z = cv.out$validation$adj %>% which.min()

est.test.err = cv.out$validation$adj %>% min() %>% round(digits = 4)

print(paste("The estimated mean squared test error for a partial least square regression model =",
            est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal number partial least square directions = ", opt.z))

```

####    Best Subset Selection - Forward Selection

1.  We perform a 10-fold cross validation to select the optimal size of the subset of features.
2.  The train data-set will be divided into 2 subsets - **train** and **test** - in the ratio 70:30. The *test subset* will be used to estimate the likely test error for a model buit using the optimal size of subset of features.

```{r regsubsets-forward}
no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)

est.val.err = rep(0, p)
for (i in 1:no.of.folds) {
    fit = regsubsets(log(SalePrice)~., data = train[indx != i, ], nvmax = p, method = "forward")
    
    test.mat = model.matrix(SalePrice~., data = train[indx == i,])
    
    for (j in 1:p) {
       coefj = coef(fit, id = j)
       y.val.hat = test.mat[, names(coefj)]%*%coefj
       val.err = (y.val.hat - y[indx == i])^2 %>% mean()
       est.val.err[j] = est.val.err[j] + val.err 
    }
    
}

est.val.err = est.val.err/no.of.folds

opt.z = which.min(est.val.err)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = regsubsets(log(SalePrice)~., data = train[indx, ], nvmax = opt.z, method = "forward")
    coef.fit = coef(fit, id = opt.z)
    test.mat = model.matrix(SalePrice~., data = train[-indx,])
    
    y.val.hat = test.mat[, names(coef.fit)]%*%coef.fit
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a best subset (forward) selection model =", est.test.err))

```

####    Best Subset Selection - Backward Selection

1.  We use the same strategy as for the *Forward Selection* algorithm.

```{r regsubsets-backward}
no.of.folds = 10
set.seed(1970)
indx = sample(1:no.of.folds, n, replace = TRUE)

est.val.err = rep(0, p)
for (i in 1:no.of.folds) {
    fit = regsubsets(log(SalePrice)~., data = train[indx != i, ], nvmax = p, method = "backward")
    
    test.mat = model.matrix(SalePrice~., data = train[indx == i,])
    
    for (j in 1:p) {
       coefj = coef(fit, id = j)
       y.val.hat = test.mat[, names(coefj)]%*%coefj
       val.err = (y.val.hat - y[indx == i])^2 %>% mean()
       est.val.err[j] = est.val.err[j] + val.err 
    }
    
}

est.val.err = est.val.err/no.of.folds

opt.z = which.min(est.val.err)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:n, 0.7*n)
    
    fit = regsubsets(log(SalePrice)~., data = train[indx, ], nvmax = opt.z, method = "backward")
    coef.fit = coef(fit, id = opt.z)
    test.mat = model.matrix(SalePrice~., data = train[-indx,])
    
    y.val.hat = test.mat[, names(coef.fit)]%*%coef.fit
    test.err = (y.val.hat - y[-indx])^2 %>% mean()
    
    est.test.err = est.test.err + test.err
}

est.test.err = round(est.test.err/5, digits = 4)

print(paste("The estimated mean squared test error for a best subset (backward) selection model =", est.test.err))

```

*Observations*
1.  None of the dimensionality reducing regression models outperform the penalised regression models.

```{r baseline-4, echo=TRUE}
#   Cleanup
rm(list = ls())

```

### Tree Based Models

####    Introduction

1.  We will use the following 2 tree-based learning algorithms:
    a.  Random Forest Ensemble
    b.  Boosted Tree Ensemble
2.  We will use the *train-validate-test* strategy to fit an optimal model to the *train* data-set as well as estimate the likely test error.



####    Getting the Data

1.  Setting up the required data.

```{r get-data-5, echo=TRUE}
train = readRDS("./RDA/train")

n = nrow(train)
p = ncol(train)-1
y = train$SalePrice %>% log()
```

####    Random Forest Ensemble

1.  Since the *random forest* algorithm uses a *bootstrap* sampling strategy, the function `randomforest()` offers an easy built-in way to estimate test error without the need for an explicit *train-test* strategy.
2.  We iterate through different values of mumber of variables randomly sampled as candidates at each split allowing us to compare a random forest ensemble with a bagged tree ensemble in the same pass.
3.  For the two parameters to optimise we use the following sets
    a.  *number of trees grown*  - {100, 500, 2500}, and 
    b.  *number of features randomly chosen for a node split* - {p/9, p/3, p}, where is p is the number of features.
4.  (From the help file) For large data sets, especially those with large number of variables, calling randomForest via the formula interface is not advised: There may be too much overhead in handling the formula.

```{r randomforest}
x = model.matrix(SalePrice~., data = train)

forest.size = c(100, 500, 2500)
param.size = c(ncol(x)/9, ncol(x)/3, ncol(x)) %>% round(digits = 0)

est.test.err = (mean(y) - y)^2 %>% mean()
bestforestsize = 500; bestparamsize = ncol(x)/3
for (t in forest.size) {
    for (p in param.size) {
        set.seed(p)
        fit = randomForest(x, y, ntree = t, mtry = p, keep.forest = FALSE)
        test.err = fit$mse %>% min()
        if (test.err < est.test.err) {
            est.test.err = test.err
            bestforestsize = fit$mse %>% which.min()
            bestparamsize = p
        }
    }
}

est.test.err = round(est.test.err, digits = 4)

print(paste("The estimated mean squared test error for a random forest ensemble model =", est.test.err))
print(paste("For the least mean squared test errror, the optimal number of trees grown is", bestforestsize,
            "and the optimal number of parameters (for the model matrix) sampled at a node is", bestparamsize))

```

*Observations*

1.  In this particular instance, the random forest ensemble outperforms the bagged tree ensemble.

####    Boosted Tree Ensemble

1.  The function `gbm()` offers an in-built *n*-fold cross validation functionality that allows for an easy way to estimate test error without the need for an explicit *train-test* strategy.
2.  We iterate through a series of values for the parameters `interaction depth` and `learning rate` to identify the boosted tree ensemble model with the least estimated mean squared test error.
3.  For the two parameters to optimise we use the following sets
    a.  *interaction depth*  - {1, 2, 3, 4}, and 
    b.  *learning rate* - {1, 0.3, 0.1, 0.03, 0.1}

```{r boosted-trees}
depth = 1:4
alpha = c(1, 0.3, 0.1, 0.03, 0.01)

est.test.err = (mean(y) - y)^2 %>% mean()
bestdepth = 1; bestalpha = 0.1
for (d in depth) {
    for (a in alpha) {
        set.seed(a)
        fit = gbm(log(SalePrice)~., data = train, distribution = "gaussian", n.trees = 2500, 
          interaction.depth = d, shrinkage = a, cv.folds = 10, keep.data = FALSE)
        
        test.err = fit$cv.error %>% min()
        if (test.err < est.test.err) {
            est.test.err = test.err
            bestdepth = d
            bestalpha = a
        }
    }
}

est.test.err = round(est.test.err, digits = 4)

print(paste("The estimated mean squared test error for a boosted forest ensemble model =", est.test.err))
print(paste("For the model with lowest mean squared test errror, the optimal interaction depth is", bestdepth,
            "and the optimal learning rate is", bestalpha))


```

*Observations*

1.  While the *Random Forest Ensemble* does not outperform the penalised regression models, the *Boosted Tree Ensemble* certainly does.

```{r baseline-5, echo=TRUE}
#   Cleanup
rm(list = ls())

```

### Predictions

####    Introduction

1.  Comparing the multiple learning algorithms employed using the estimated mean squared test error as a measure of success, we shortlist the following to predict the `SalePrice` for the *test* dataset.
    a.  Ridge Penalised Linear Regression (est. test set error = 0.0174)
    b.  Lasso Penalised Linear Regression (est. test set error = 0.0176)
    c.  Partial Least Square Regression (estimated test set error = 0.0587)
    d.  Boosted Tree Ensemble (estimated test set error = 0.015)
2.  We are including the Partial Least Square Regression Model to ensure that it performs as poorly on the test dataset as it does on the train set, thereby validating the approach taken through this entire project.

####    Getting the Data

1.  Setting up the required data.

```{r get-data-6, echo=TRUE}
train = readRDS("./RDA/train")
test = readRDS("./RDA/test")
n.train = nrow(train)
n.test = nrow(test)
p = ncol(train)-1
y.train = train$SalePrice %>% log()
x.train = model.matrix(SalePrice~., data = train)[,-1]
x.test = model.matrix(SalePrice~., data = test)[,-1]

```

####    Ridge Penalised Linear Regression - Predictions

1.  (From the help file) The results of `cv.glmnet` are random, since the folds are selected at random. We will therefore run `cv.glmnet` 5 times, for each iteration compute the prediction for the test set and then average the prediction.

```{r ridge-pred}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = glmnet(x.train, y.train, family = "gaussian", alpha = 1)
    bestlambda = cv.glmnet(x.train, y.train, type.measure = "mse", alpha = 1)$lambda.min
        
    y.test.hat = y.test.hat + predict(fit, newx = x.test, s = bestlambda, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)

```


```{r ridge-pred-write, echo=TRUE}
write.table(y.test.hat, file = "./data/out-ridge.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Lasso Penalised Linear Regression Predictions

1.  We use the same strategy as for the Ridge Penalised model

```{r lasso-pred}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = glmnet(x.train, y.train, family = "gaussian", alpha = 0)
    bestlambda = cv.glmnet(x.train, y.train, type.measure = "mse", alpha = 0)$lambda.min
        
    y.test.hat = y.test.hat + predict(fit, newx = x.test, s = bestlambda, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)

```


```{r lasso-pred-write, echo=TRUE}
write.table(y.test.hat, file = "./data/out-lasso.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Partial Least Squares Regression

1.  The Partial Least Squares Regression requires scaling of the numeric features (ref. discussion earlier on the same topic)
2.  The results of `cross.val` are random, since the folds are selected at random. We will therefore run `cross.val` 5 times, for each iteration compute the prediction for the test set and then average the prediction.
3.  We scale the *test* dataset features to the same scale as the respective feature in the *train* dataset

```{r pls-scaling-pred}
train.scaled = train
train.scaled$SalePrice = log(train$SalePrice)

num.feat.indx = sapply(train, is.numeric)
num.feat.rnge = sapply(train[, num.feat.indx], range)

num.feat.like.fct = c("BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", 
                      "Fireplaces", "GarageCars")
num.feat.indx[num.feat.like.fct] = FALSE
train.scale.centers = apply(train.scaled[, num.feat.indx], 2, mean)
train.scale.sdev = apply(train.scaled[, num.feat.indx], 2, sd)

train.scaled[, num.feat.indx] = apply(train.scaled[, num.feat.indx], 2, scale)


test.scaled = test
for (i in 1:length(num.feat.indx)) {
    if (num.feat.indx[i]) {
        feat = names(num.feat.indx[i])
        test.scaled[, feat] = scale(test.scaled[, feat], center = train.scale.centers[feat], 
                                    scale = train.scale.sdev[feat])
    }
}

#   "Unlist" the scaled features to drop the attributes of the scaling stored in the dataframe
test.scaled[, num.feat.indx] = unlist(test.scaled[, num.feat.indx])

```


```{r pls-pred}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = plsr(log(SalePrice)~., data = train.scaled, ncomp = p, center = FALSE, validation = "none", 
               model = FALSE)
    
    set.seed(i)
    cv.out = crossval(fit, segments = 10, segment.type = "random")
    opt.z = cv.out$validation$adj %>% which.min()
    
    y.test.hat = y.test.hat + predict(fit, newdata = test.scaled, comps = opt.z, type = "response")
}

y.test.hat = y.test.hat/5

#   "Unscaling" the dependent variable
y.test.hat = y.test.hat*train.scale.sdev["SalePrice"] + train.scale.centers["SalePrice"]
y.test.hat = exp(y.test.hat)

```


```{r pls-pred-write, echo=TRUE}
write.table(y.test.hat, file = "./data/out-pls.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Boosted Tree Ensemble

1.  We use the optimal interaction depth (= 4) and optimal learning rate (= 0.01) determined earlier through cross validation for the lowest estimated test error.

```{r boosted-trees-pred}
fit = gbm(log(SalePrice)~., data = train, distribution = "gaussian", n.trees = 2500, 
          interaction.depth = 4, shrinkage = 0.01, cv.folds = 0, keep.data = TRUE)

y.test.hat = predict(fit, newdata = test, n.trees = 2500, type = "response")
y.test.hat = exp(y.test.hat)

```


```{r boosted-trees-pred-write, echo=TRUE}
write.table(y.test.hat, file = "./data/out-gbm.csv", quote = FALSE, sep = ",",
            row.names = rownames(test), col.names = "Id,SalePrice")

```

### Results

1.  We used four models to make predictions for the test dataset. The results, from the Kaggle evaluation, are as follows:

**Model                               Est. Test Error             Act. Test Scores**

Boosted Trees ensemble                  0.0150                      0.12555

Ridge-penalised Regression              0.0174                      0.12966

Lasso-penalised Regression              0.0176                      0.13044

Partial Least Square Regression         0.0587                      0.41967


2.  It is quite satisfactory that the actual ranking of the models by test scores mirrors the ranking of the models by estimated test error validating the approach used in evaluating the different models

