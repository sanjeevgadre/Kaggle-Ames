---
title: "Ames Housing Prices: Part VI - Predictions"
author: "Sanjeev Gadre"
date: "June 28, 2019"
output: 
    md_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE) #message = FALSE
```

Loading the required libraries.

```{r libraries, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(glmnet)
library(pls)
library(randomForest)
library(gbm)

```



### Getting the Data

1.  We start by getting the previously cleaned *train* subset.

```{r get-data}
train = readRDS("./RDA/train")
test = readRDS("./RDA/test")

```

### Introduction

1.  Comparing the multiple learning algorithms employed using the estimated mean squared test error as a measure of success, we shortlist the following to predict the `SalePrice` for the *test* set.
    a.  Ridge Penalised Linear Regression (est. test set error = 0.0171)
    b.  Lasso Penalised Linear Regression (est. test set error = 0.0172)
    c.  Principal Component Regression (est. test set error = 0.0178)
    d.  Partial Least Square Regression (estimated test set error = 0.0094)
    e.  Boosted Tree Ensemble (estimated test set error = 0.015)

```{r common}
n.train = nrow(train)
n.test = nrow(test)
p = ncol(train)-1
y.train = train$SalePrice %>% log()
x.train = model.matrix(SalePrice~., data = train)[,-1]
x.test = model.matrix(SalePrice~., data = test)[,-1]

```

####    Ridge Penalised Linear Regression - Predictions

1.  (From the help file) The results of `cv.glmnet` are random, since the folds are selected at random. We will therefore run `cv.glmnet` 5 times, for each iteration compute the prediction for the test set and then average the prediction.

```{r ridge}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = glmnet(x.train, y.train, family = "gaussian", alpha = 1)
    bestlambda = cv.glmnet(x.train, y.train, type.measure = "mse", alpha = 1)$lambda.min
        
    y.test.hat = y.test.hat + predict(fit, newx = x.test, s = bestlambda, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)


write.table(y.test.hat, file = "./data/out-ridge.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")


```

####    Lasso Penalised Linear Regression Predictions

1.  We use the same strategy as for the Ridge Penalised model

```{r lasso}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = glmnet(x.train, y.train, family = "gaussian", alpha = 0)
    bestlambda = cv.glmnet(x.train, y.train, type.measure = "mse", alpha = 0)$lambda.min
        
    y.test.hat = y.test.hat + predict(fit, newx = x.test, s = bestlambda, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)


write.table(y.test.hat, file = "./data/out-lasso.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Principal Component Regression

1.  The results of `cross.val` are random, since the folds are selected at random. We will therefore run `cross.val` 5 times, for each iteration compute the prediction for the test set and then average the prediction.

```{r pcr}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = pcr(log(SalePrice)~., data = train, ncomp = p, center = TRUE, validation = "none", model = FALSE)
    
    set.seed(i)
    cv.out = crossval(fit, segments = 10, segment.type = "random")
    opt.z = cv.out$validation$adj %>% which.min()
    
    y.test.hat = y.test.hat + predict(fit, newdata = test, comps = opt.z, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)

write.table(y.test.hat, file = "./data/out-pcr.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Partial Least Squares Regression

1.  The results of `cross.val` are random, since the folds are selected at random. We will therefore run `cross.val` 5 times, for each iteration compute the prediction for the test set and then average the prediction.

```{r pls}
y.test.hat = rep(0, n.test)
for (i in 1:5) {
    set.seed(i)
    fit = plsr(log(SalePrice)~., data = train, ncomp = p, center = TRUE, validation = "none", model = FALSE)
    
    set.seed(i)
    cv.out = crossval(fit, segments = 10, segment.type = "random")
    opt.z = cv.out$validation$adj %>% which.min()
    
    y.test.hat = y.test.hat + predict(fit, newdata = test, comps = opt.z, type = "response")
}

y.test.hat = y.test.hat/5 
y.test.hat = exp(y.test.hat)

write.table(y.test.hat, file = "./data/out-pls.csv", quote = FALSE, sep = ",",
            row.names = TRUE, col.names = "Id,SalePrice")

```

####    Boosted Tree Ensemble

1.  We use the optimal interaction depth (= 4) and optimal learning rate (= 0.01) determined earlier through cross validation for the lowest estimated test error.

```{r boosted-trees}
fit = gbm(log(SalePrice)~., data = train, distribution = "gaussian", n.trees = 2500, 
          interaction.depth = 4, shrinkage = 0.01, cv.folds = 0, keep.data = TRUE)

y.test.hat = predict(fit, newdata = test, n.trees = 2500, type = "response")
y.test.hat = exp(y.test.hat)

write.table(y.test.hat, file = "./data/out-gbm.csv", quote = FALSE, sep = ",",
            row.names = rownames(test), col.names = "Id,SalePrice")

```

